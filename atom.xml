<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>陶光品的博客</title>
  
  <subtitle>Better than Yesterday</subtitle>
  <link href="https://guangpintao.github.io/atom.xml" rel="self"/>
  
  <link href="https://guangpintao.github.io/"/>
  <updated>2024-03-20T09:02:44.375Z</updated>
  <id>https://guangpintao.github.io/</id>
  
  <author>
    <name>Guangpin Tao</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Coarse-to-Fine Amodal Segmentation with Shape Prior</title>
    <link href="https://guangpintao.github.io/2024/03/20/Coarse-to-Fine-Amodal-Segmentation-with-Shape-Prior/"/>
    <id>https://guangpintao.github.io/2024/03/20/Coarse-to-Fine-Amodal-Segmentation-with-Shape-Prior/</id>
    <published>2024-03-20T06:32:37.000Z</published>
    <updated>2024-03-20T09:02:44.375Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Coarse-to-Fine-Amodal-Segmentation-with-Shape-Prior"><a href="#Coarse-to-Fine-Amodal-Segmentation-with-Shape-Prior" class="headerlink" title="Coarse-to-Fine Amodal Segmentation with Shape Prior"></a>Coarse-to-Fine Amodal Segmentation with Shape Prior</h2><p>论文做的任务是Amodel Segmentation（无模态分割，或者部分遮挡补全分割），最初的一篇论文是CVPR2017年FAIR提出的<a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhu_Semantic_Amodal_Segmentation_CVPR_2017_paper.pdf">Semantic Amodal Segmentation</a>，在分割目标的同时脑补出遮挡目标的完整形状轮廓。（<strong>outpaint</strong>+<strong>semseg</strong>）<br><br></p><blockquote><p>Amodal object segmentation aims to segment not only the visible parts of an object but also its occluded parts.</p></blockquote><p><img src="/img/image_6.png" alt="alt text"><br><br><br>VM是目标可见区域的mask，GT是Amodel Segmask，分割的目标是完成图像中目标可能存在区域的识别。</p><h4 id="论文贡献"><a href="#论文贡献" class="headerlink" title="论文贡献"></a>论文贡献</h4><ul><li>Reduces the learning space from the pixel-level image space to the vector-quantized latent space, learn a <strong>coarse-grained</strong> amodal segment to  handle <strong>long-range</strong> dependencies.</li><li>Propose a convolution refine module to inject <strong>fine-grained</strong> information and<br>provide a more precise amodal object segmentation based on visual features and coarse-predicted segmentation</li><li>Create a <strong>synthetic</strong> amodal dataset, named as <strong>MOViD-Amodal</strong> (MOViD-A), which can be used for both image and video amodal object segmentation.</li><li>Evaluation on two benchmark datasets: KINS and COCO-A demonstrate the superiority of C2FSeg</li></ul><h4 id="方法框架"><a href="#方法框架" class="headerlink" title="方法框架"></a>方法框架</h4><p><img src="/img/image-7.png" alt="alt text"></p><center>VQ重建方式根据可见mask和可见视觉特征生成coarse mask, 使用可见特征上的基于卷积attention refine coarse mask </center><h4 id="方法细节"><a href="#方法细节" class="headerlink" title="方法细节"></a>方法细节</h4><p><img src="/img/image-8.png" alt="alt text"></p><ol><li>使用<strong>Vector-Quantized Latent Space</strong>mask结构先验，使用训练好的Encoder latent表示+index embedding:<br><script type="math/tex">v_{M}=Embed(q(E(M)))</script> 作为transformer的输入， 论文说使用现有的VQ-GAN将图像编码到隐空间性能下降，原因是已有VQ-GAN的训练数据与AS不一致，论文采用预训练的ResNet得到图像latent表示作为tranformer输入。</li><li><p>基于上述$v_{img}$和可见分割表示$v_{\hat{M}_v}$（通过附加学习任务得到）自回归预测$v_{M_{a}}$。预测的输入是$cat(v_{img},v_{\hat{M}_v},\hat{v}$) ，$\hat{v}$是跟$v_{\hat{M}_v}$相同维度的全[MASK] token。</p></li><li><p>损失函数：最大似然参数估计采用最小化NLL损失，论文采用了<strong>Mask-and-Predict</strong>逐步估计的策略，即输入的$\hat{v}$是$v_{M_a}$的随机 $50\%-100\%$mask结果，测试时采用迭代K步，每步保留最置信token的方式, 使用VQ-Decoder从$\hat{v}$解码出估计的$\hat{M}_c$</p></li><li></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Coarse-to-Fine-Amodal-Segmentation-with-Shape-Prior&quot;&gt;&lt;a href=&quot;#Coarse-to-Fine-Amodal-Segmentation-with-Shape-Prior&quot; class=&quot;headerlin</summary>
      
    
    
    
    
    <category term="ICCV2023" scheme="https://guangpintao.github.io/tags/ICCV2023/"/>
    
  </entry>
  
  <entry>
    <title>VRPSAM--SAM with Visual Reference Prompt</title>
    <link href="https://guangpintao.github.io/2024/03/18/VRP-SAM-CVPR2024/"/>
    <id>https://guangpintao.github.io/2024/03/18/VRP-SAM-CVPR2024/</id>
    <published>2024-03-18T14:54:26.000Z</published>
    <updated>2024-03-19T12:47:33.582Z</updated>
    
    <content type="html"><![CDATA[<h1 id="VRP-SAM-SAM-with-Visual-Reference-Prompt"><a href="#VRP-SAM-SAM-with-Visual-Reference-Prompt" class="headerlink" title="VRP-SAM: SAM with Visual Reference Prompt"></a><a href="https://arxiv.org/pdf/2402.17726">VRP-SAM: SAM with Visual Reference Prompt</a></h1><p><img src="/img/image-2.png" alt=""></p><h2 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h2><p>Empowers the Segment Anything Model (SAM) to utilize annotated reference images as prompts for segmentation.</p><p>Visual reference segmentation with minimal learnable parameters and strong generalization capabilities.</p><h2 id="提出方法"><a href="#提出方法" class="headerlink" title="提出方法"></a>提出方法</h2><p><img src="/img/image.png" alt=""></p><center>VRPSAM 接受point, scribble, box或者mask标注好的参考图像 </center><p><img src="/img/image-1.png" alt=""></p><center>Visual reference Prompt Encoder包括Feature Augmenter和Prompt Generator</center><p><br><br>Feature Augmenter中 Image encoder是固定的ResNet50, 使用reference的MaskAvgPool特征和前景mask作为提示, Prompt Generator 使用N个VRP quries首先和参考图像做交互学习分割目标知识，然后和目标图像做交互获得前景信息，最后经SA得到和SAM对齐的Prompts（听上去很玄乎）</p><h2 id="方法细节"><a href="#方法细节" class="headerlink" title="方法细节"></a>方法细节</h2><p>二分类损失和Dice Loss作为损失函数:<br><img src="/img/image-3.png" alt=""><br><br><br>实验设置： </p><ul><li>Few-shot setting on COCO-20<sup>i</sup>（60 train base+20 test novel） and Pascal-5<sup>i</sup> datasets(15 train base+5 test novel), randomly sampled 1000 reference-target pairs in each fold for evaluation.</li><li>使用SEEM生成point, cribble和box annotations.</li></ul><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="/img/image-4.png" alt=""><br><img src="/img/image-5.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;VRP-SAM-SAM-with-Visual-Reference-Prompt&quot;&gt;&lt;a href=&quot;#VRP-SAM-SAM-with-Visual-Reference-Prompt&quot; class=&quot;headerlink&quot; title=&quot;VRP-SAM: SAM</summary>
      
    
    
    
    
    <category term="CVPR2024" scheme="https://guangpintao.github.io/tags/CVPR2024/"/>
    
  </entry>
  
</feed>
