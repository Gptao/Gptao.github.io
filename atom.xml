<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>陶光品的博客</title>
  
  <subtitle>Better than Yesterday</subtitle>
  <link href="https://guangpintao.github.io/atom.xml" rel="self"/>
  
  <link href="https://guangpintao.github.io/"/>
  <updated>2024-03-21T08:06:02.086Z</updated>
  <id>https://guangpintao.github.io/</id>
  
  <author>
    <name>Guangpin Tao</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Images Speak in Images: A Generalist Painter for In-Context Visual Learning</title>
    <link href="https://guangpintao.github.io/2024/03/21/Images-Speak-in-Images-A-Generalist-Painter-for-In-Context-Visual-Learning/"/>
    <id>https://guangpintao.github.io/2024/03/21/Images-Speak-in-Images-A-Generalist-Painter-for-In-Context-Visual-Learning/</id>
    <published>2024-03-21T04:01:16.000Z</published>
    <updated>2024-03-21T08:06:02.086Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Images_Speak_in_Images_A_Generalist_Painter_for_In-Context_Visual_CVPR_2023_paper.pdf">Images Speak in Images: A Generalist Painter for In-Context Visual Learning</a></p><blockquote><p>首次使用inpainting方式做visual in-context learning:<br><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/9f09f316a3eaf59d9ced5ffaefe97e0f-Paper-Conference.pdf">Visual prompting via image inpainting (NeurIPS2022)</a></p><h4 id="论文贡献"><a href="#论文贡献" class="headerlink" title="论文贡献"></a>论文贡献</h4><ul><li>提出视觉任务的<strong>In-Context Learning</strong>， 使用图像定义视觉任务输出的统一表示</li><li>对输出输出图像对做mask image modeling, 使用Vision-prompt形式决定推理时任务</li><li>在包括深度估计，语义分割，人体关键点定位，全景分割等任务上均取得了比较好的效果</li></ul><p>we believe that images speak in images, i.e., image itself is a natural interface for general-purpose visual perception.  The core observation is that most dense-prediction vision problems can be formulated as <strong>image inpainting</strong></p></blockquote><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><p>NYUv2+ADE20K+MS-COCO+SIDD+LoL等等</p><h4 id="方法框架"><a href="#方法框架" class="headerlink" title="方法框架"></a>方法框架</h4><p><img src="/img/image-10.png" alt="alt text"><br>使用masked image modeling的方式训练patch<strong>自回归模型</strong>，使用ViT-large模型，masked 区域替换为可学习token vector, mask 70$\%$, 实际为了减小计算量，不是四个图拼接在一起，而是把并行输出的四张图在固定蹭特征拼在一起， 输出使用smooth-$\ell_1$监督， 训练需要8nodes*8GPU (total_bsz = 8x8x32 = 2048).</p><h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><p><img src="/img/image-11.png" alt="alt text"><br><img src="/img/image-12.png" alt="alt text"></p><h4 id="有价值点"><a href="#有价值点" class="headerlink" title="有价值点"></a>有价值点</h4><p>论证了MIM+patch自回归强大的图像重建能力</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2023/papers/Wang_Images_Speak_in_Images_A_Generalist_Painter_for_In-Context_Visual_CVP</summary>
      
    
    
    
    
    <category term="CVPR2023" scheme="https://guangpintao.github.io/tags/CVPR2023/"/>
    
  </entry>
  
  <entry>
    <title>SegGPT: Segmenting Everything In Context</title>
    <link href="https://guangpintao.github.io/2024/03/21/SegGPT-Segmenting-Everything-In-Context/"/>
    <id>https://guangpintao.github.io/2024/03/21/SegGPT-Segmenting-Everything-In-Context/</id>
    <published>2024-03-21T03:58:09.000Z</published>
    <updated>2024-03-21T08:07:21.630Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>基于Painter在分割领域做的专家系统: segmenting everything in context with only one single model, which uses in-context examples to indicate different tasks</p></blockquote><h4 id="论文贡献"><a href="#论文贡献" class="headerlink" title="论文贡献"></a>论文贡献</h4><ul><li>首次论证使用单个模型可以自动处理多种类型的分割任务</li><li>在大量下游任务上验证pre-trained SegGPT 具有强大的分割能力</li></ul><h4 id="具体方法"><a href="#具体方法" class="headerlink" title="具体方法"></a>具体方法</h4><ul><li>基于Painter设计通用分割任务：<br>Context是具体的分割方式，部分|语义|实例|全景， 输出依然是coloring模式, 支持multi-example 模式</li><li>如何消除Context之间歧义？</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;基于Painter在分割领域做的专家系统: segmenting everything in context with only one single model, which uses in-context examples to indicat</summary>
      
    
    
    
    
    <category term="ICCV2023" scheme="https://guangpintao.github.io/tags/ICCV2023/"/>
    
  </entry>
  
  <entry>
    <title>Coarse-to-Fine Amodal Segmentation with Shape Prior</title>
    <link href="https://guangpintao.github.io/2024/03/20/Coarse-to-Fine-Amodal-Segmentation-with-Shape-Prior/"/>
    <id>https://guangpintao.github.io/2024/03/20/Coarse-to-Fine-Amodal-Segmentation-with-Shape-Prior/</id>
    <published>2024-03-20T06:32:37.000Z</published>
    <updated>2024-03-21T07:51:15.006Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Coarse-to-Fine-Amodal-Segmentation-with-Shape-Prior"><a href="#Coarse-to-Fine-Amodal-Segmentation-with-Shape-Prior" class="headerlink" title="Coarse-to-Fine Amodal Segmentation with Shape Prior"></a><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_Coarse-to-Fine_Amodal_Segmentation_with_Shape_Prior_ICCV_2023_paper.pdf">Coarse-to-Fine Amodal Segmentation with Shape Prior</a></h2><p><a href="https://github.com/amazon-science/c2f-seg">git</a></p><p>论文做的任务是Amodal Segmentation（无模态分割，或者部分遮挡补全分割），最初的一篇论文是CVPR2017年FAIR提出的<a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhu_Semantic_Amodal_Segmentation_CVPR_2017_paper.pdf">Semantic Amodal Segmentation</a>，在分割目标的同时脑补出遮挡目标的完整形状轮廓。（<strong>outpaint</strong>+<strong>semseg</strong>）<br><br></p><blockquote><p>Amodal object segmentation aims to segment not only the visible parts of an object but also its occluded parts.</p></blockquote><p><img src="/img/image_6.png" alt="alt text"><br><br><br>VM是目标可见区域的mask，GT是Amodal Segmask，分割的目标是完成图像中目标可能存在区域的识别。</p><h4 id="论文贡献"><a href="#论文贡献" class="headerlink" title="论文贡献"></a>论文贡献</h4><ul><li>Reduces the learning space from the pixel-level image space to the vector-quantized latent space, learn a <strong>coarse-grained</strong> amodal segment to  handle <strong>long-range</strong> dependencies.</li><li>Propose a convolution refine module to inject <strong>fine-grained</strong> information and<br>provide a more precise amodal object segmentation based on visual features and coarse-predicted segmentation</li><li>Create a <strong>synthetic</strong> amodal dataset, named as <strong>MOViD-Amodal</strong> (MOViD-A), which can be used for both image and video amodal object segmentation.</li><li>Evaluation on two benchmark datasets: KINS and COCO-A demonstrate the superiority of C2FSeg</li></ul><h4 id="方法框架"><a href="#方法框架" class="headerlink" title="方法框架"></a>方法框架</h4><p><img src="/img/image-7.png" alt="alt text"></p><center>VQ重建方式根据可见mask和可见视觉特征生成coarse mask, 使用可见特征上的基于卷积attention refine coarse mask </center><h4 id="方法细节"><a href="#方法细节" class="headerlink" title="方法细节"></a>方法细节</h4><p><img src="/img/image-8.png" alt="alt text"></p><ol><li><p>使用<strong>Vector-Quantized Latent Space</strong>mask结构先验，使用训练好的Encoder latent表示 + index embedding: $v_{M}=Embed(q(E(M)))$ 作为transformer的输入， 论文说使用现有的VQ-GAN将图像编码到隐空间性能下降，原因是已有VQ-GAN的训练数据与AS不一致，论文采用预训练的ResNet得到图像latent表示作为tranformer输入。</p></li><li><p>基于上述$v_{img}$和可见分割表示$v_{\hat{M}_v}$（通过附加学习任务得到）自回归预测$v_{M_{a}}$。预测的输入是$cat(v_{img},v_{\hat{M}_v},\hat{v}$) ，$\hat{v}$是跟$v_{\hat{M}_v}$相同维度的全[MASK] token。</p></li><li><p>损失函数：最大似然参数估计采用最小化NLL损失，论文采用了<strong>Mask-and-Predict</strong>逐步估计的策略，即输入的$\hat{v}$是$v_{M_a}$的随机 $50\%-100\%$mask结果，测试时采用迭代K步，每步保留最置信token的方式, 使用VQ-Decoder从$\hat{v}$解码出估计的$\hat{M}_c$</p></li><li>卷积refine模块的输入是$v_{img}$和$\hat{M}_c$，下采样$\hat{M}_c$， 综合resnet特征和$\hat{M}_c$与$v_{img}$相似度使用卷积得到refined $\hat{M}_a$和$\hat{M}_c$，分别用BCE监督。</li></ol><p><img src="/img/image-9.png" alt="alt text"></p><blockquote><p>同一个像素可以属于两个不同的类别，还是只关注遮挡目标？ </p></blockquote><p><strong>论文提出的C2F-Seg方法并不具有检测能力，需要依赖现有的可见实例分割（AISFormer）的结果进行补全和指标计算。</strong></p><h4 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h4><p>数据集： KINS，COCOA， FISHBOWL(video)， MOViD-A(Video)</p><h6 id="有价值的地方：-VQGAN的实现和使用"><a href="#有价值的地方：-VQGAN的实现和使用" class="headerlink" title="有价值的地方： VQGAN的实现和使用"></a>有价值的地方： VQGAN的实现和使用</h6>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Coarse-to-Fine-Amodal-Segmentation-with-Shape-Prior&quot;&gt;&lt;a href=&quot;#Coarse-to-Fine-Amodal-Segmentation-with-Shape-Prior&quot; class=&quot;headerlin</summary>
      
    
    
    
    
    <category term="ICCV2023" scheme="https://guangpintao.github.io/tags/ICCV2023/"/>
    
  </entry>
  
  <entry>
    <title>VRPSAM--SAM with Visual Reference Prompt</title>
    <link href="https://guangpintao.github.io/2024/03/18/VRP-SAM-CVPR2024/"/>
    <id>https://guangpintao.github.io/2024/03/18/VRP-SAM-CVPR2024/</id>
    <published>2024-03-18T14:54:26.000Z</published>
    <updated>2024-03-19T12:47:33.582Z</updated>
    
    <content type="html"><![CDATA[<h1 id="VRP-SAM-SAM-with-Visual-Reference-Prompt"><a href="#VRP-SAM-SAM-with-Visual-Reference-Prompt" class="headerlink" title="VRP-SAM: SAM with Visual Reference Prompt"></a><a href="https://arxiv.org/pdf/2402.17726">VRP-SAM: SAM with Visual Reference Prompt</a></h1><p><img src="/img/image-2.png" alt=""></p><h2 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h2><p>Empowers the Segment Anything Model (SAM) to utilize annotated reference images as prompts for segmentation.</p><p>Visual reference segmentation with minimal learnable parameters and strong generalization capabilities.</p><h2 id="提出方法"><a href="#提出方法" class="headerlink" title="提出方法"></a>提出方法</h2><p><img src="/img/image.png" alt=""></p><center>VRPSAM 接受point, scribble, box或者mask标注好的参考图像 </center><p><img src="/img/image-1.png" alt=""></p><center>Visual reference Prompt Encoder包括Feature Augmenter和Prompt Generator</center><p><br><br>Feature Augmenter中 Image encoder是固定的ResNet50, 使用reference的MaskAvgPool特征和前景mask作为提示, Prompt Generator 使用N个VRP quries首先和参考图像做交互学习分割目标知识，然后和目标图像做交互获得前景信息，最后经SA得到和SAM对齐的Prompts（听上去很玄乎）</p><h2 id="方法细节"><a href="#方法细节" class="headerlink" title="方法细节"></a>方法细节</h2><p>二分类损失和Dice Loss作为损失函数:<br><img src="/img/image-3.png" alt=""><br><br><br>实验设置： </p><ul><li>Few-shot setting on COCO-20<sup>i</sup>（60 train base+20 test novel） and Pascal-5<sup>i</sup> datasets(15 train base+5 test novel), randomly sampled 1000 reference-target pairs in each fold for evaluation.</li><li>使用SEEM生成point, cribble和box annotations.</li></ul><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="/img/image-4.png" alt=""><br><img src="/img/image-5.png" alt=""></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;VRP-SAM-SAM-with-Visual-Reference-Prompt&quot;&gt;&lt;a href=&quot;#VRP-SAM-SAM-with-Visual-Reference-Prompt&quot; class=&quot;headerlink&quot; title=&quot;VRP-SAM: SAM</summary>
      
    
    
    
    
    <category term="CVPR2024" scheme="https://guangpintao.github.io/tags/CVPR2024/"/>
    
  </entry>
  
</feed>
