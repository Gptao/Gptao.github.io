[{"title":"Coarse-to-Fine Amodal Segmentation with Shape Prior","url":"/2024/03/20/Coarse-to-Fine-Amodal-Segmentation-with-Shape-Prior/","content":"\n## Coarse-to-Fine Amodal Segmentation with Shape Prior\n\n论文做的任务是Amodel Segmentation（无模态分割，或者部分遮挡补全分割），最初的一篇论文是CVPR2017年FAIR提出的[Semantic Amodal Segmentation](https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhu_Semantic_Amodal_Segmentation_CVPR_2017_paper.pdf)，在分割目标的同时脑补出遮挡目标的完整形状轮廓。（**outpaint**+**semseg**）\n<br>\n>Amodal object segmentation aims to segment not only the visible parts of an object but also its occluded parts.\n\n![alt text](/img/image_6.png)\n<br>\nVM是目标可见区域的mask，GT是Amodel Segmask，分割的目标是完成图像中目标可能存在区域的识别。\n\n#### 论文贡献\n- Reduces the learning space from the pixel-level image space to the vector-quantized latent space, learn a **coarse-grained** amodal segment to  handle **long-range** dependencies.\n- Propose a convolution refine module to inject **fine-grained** information and\nprovide a more precise amodal object segmentation based on visual features and coarse-predicted segmentation\n- Create a **synthetic** amodal dataset, named as **MOViD-Amodal** (MOViD-A), which can be used for both image and video amodal object segmentation.\n- Evaluation on two benchmark datasets: KINS and COCO-A demonstrate the superiority of C2FSeg\n\n#### 方法框架\n![alt text](/img/image-7.png)\n<center>VQ重建方式根据可见mask和可见视觉特征生成coarse mask, 使用可见特征上的基于卷积attention refine coarse mask </center>\n\n#### 方法细节\n![alt text](/img/image-8.png)\n1. 使用**Vector-Quantized Latent Space**mask结构先验，使用训练好的Encoder latent表示 + index embedding: $v_{M}=Embed(q(E(M)))$ 作为transformer的输入， 论文说使用现有的VQ-GAN将图像编码到隐空间性能下降，原因是已有VQ-GAN的训练数据与AS不一致，论文采用预训练的ResNet得到图像latent表示作为tranformer输入。\n   \n2. 基于上述$v_{img}$和可见分割表示$v_{\\hat{M}_v}$（通过附加学习任务得到）自回归预测$v_{M_{a}}$。预测的输入是$cat(v_{img},v_{\\hat{M}_v},\\hat{v}$) ，$\\hat{v}$是跟$v_{\\hat{M}_v}$相同维度的全[MASK] token。\n\n3. 损失函数：最大似然参数估计采用最小化NLL损失，论文采用了**Mask-and-Predict**逐步估计的策略，即输入的$\\hat{v}$是$v_{M_a}$的随机 $50\\%-100\\%$mask结果，测试时采用迭代K步，每步保留最置信token的方式, 使用VQ-Decoder从$\\hat{v}$解码出估计的$\\hat{M}_c$\n4. 卷积refine模块的输入是$v_{img}$和$\\hat{M}_c$，下采样$\\hat{M}_c$， 综合resnet特征和$\\hat{M}_c$与$v_{img}$相似度使用卷积得到refined $\\hat{M}_a$和$\\hat{M}_c$，分别用BCE监督。\n![alt text](/img/image-9.png)\n\n> 同一个像素可以属于两个不同的类别，还是只关注遮挡目标？ \n\n#### 实验设置\n数据集： KINS，COCOA， FISHBOWL(video)， MOViD-A(Video)\n\n#### 复现一下实验代价和指标","tags":["ICCV2023"]},{"title":"VRPSAM--SAM with Visual Reference Prompt","url":"/2024/03/18/VRP-SAM-CVPR2024/","content":"\n# [VRP-SAM: SAM with Visual Reference Prompt](https://arxiv.org/pdf/2402.17726)\n![](/img/image-2.png)\n## 解决问题\nEmpowers the Segment Anything Model (SAM) to utilize annotated reference images as prompts for segmentation.\n\nVisual reference segmentation with minimal learnable parameters and strong generalization capabilities.\n\n## 提出方法\n\n![](/img/image.png)\n<center>VRPSAM 接受point, scribble, box或者mask标注好的参考图像 </center>\n\n![](/img/image-1.png)\n\n<center>Visual reference Prompt Encoder包括Feature Augmenter和Prompt Generator</center>\n\n<br>\nFeature Augmenter中 Image encoder是固定的ResNet50, 使用reference的MaskAvgPool特征和前景mask作为提示, Prompt Generator 使用N个VRP quries首先和参考图像做交互学习分割目标知识，然后和目标图像做交互获得前景信息，最后经SA得到和SAM对齐的Prompts（听上去很玄乎）\n\n## 方法细节\n二分类损失和Dice Loss作为损失函数:\n![](/img/image-3.png)\n<br>\n实验设置： \n- Few-shot setting on COCO-20<sup>i</sup>（60 train base+20 test novel） and Pascal-5<sup>i</sup> datasets(15 train base+5 test novel), randomly sampled 1000 reference-target pairs in each fold for evaluation.\n- 使用SEEM生成point, cribble和box annotations.\n\n## 实验结果\n![](/img/image-4.png)\n![](/img/image-5.png)\n\n","tags":["CVPR2024"]}]