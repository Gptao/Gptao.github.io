[{"title":"Coarse-to-Fine Amodal Segmentation with Shape Prior","url":"/2024/03/20/Coarse-to-Fine-Amodal-Segmentation-with-Shape-Prior/","content":"\n## Coarse-to-Fine Amodal Segmentation with Shape Prior\n\n论文做的任务是Amodel Segmentation（无模态分割，或者部分遮挡补全分割），最初的一篇论文是CVPR2017年FAIR提出的[Semantic Amodal Segmentation](https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhu_Semantic_Amodal_Segmentation_CVPR_2017_paper.pdf)，在分割目标的同时脑补出遮挡目标的完整形状轮廓。（**outpaint**+**semseg**）\n<br>\n>Amodal object segmentation aims to segment not only the visible parts of an object but also its occluded parts.\n\n![alt text](/img/image_6.png)\n<br>\nVM是目标可见区域的mask，GT是Amodel Segmask，分割的目标是完成图像中目标可能存在区域的识别。\n\n#### 论文贡献\n- Reduces the learning space from the pixel-level image space to the vector-quantized latent space -- learn a **coarse-grained** amodal segment to  handle **long-range** dependencies.\n- Propose a convolution refine module to inject **fine-grained** information and\nprovide a more precise amodal object segmentation based on visual features and coarse-predicted segmentation\n- Create a **synthetic** amodal dataset, named as **MOViD-Amodal** (MOViD-A), which can be used for both image and video amodal object segmentation.\n- Evaluation on two benchmark datasets: KINS and COCO-A demonstrate the superiority of C2FSeg\n\n#### 方法框架\n![alt text](/img/image-7.png)\n<center>VQ重建方式根据可见mask和可见视觉特征生成coarse mask, 使用可见特征上的基于卷积attention refine coarse mask </center>\n\n#### 方法细节\n![alt text](/img/image-8.png)\n使用**Vector-Quantized Latent Space**获得mask结构先验，但类似VQ-GAN的现有VQ方法是在大规模图像数据集上训练的，用来重建mask效果不好，","tags":["ICCV2023"]},{"title":"VRPSAM--SAM with Visual Reference Prompt","url":"/2024/03/18/VRP-SAM-CVPR2024/","content":"\n# [VRP-SAM: SAM with Visual Reference Prompt](https://arxiv.org/pdf/2402.17726)\n![](/img/image-2.png)\n## 解决问题\nEmpowers the Segment Anything Model (SAM) to utilize annotated reference images as prompts for segmentation.\n\nVisual reference segmentation with minimal learnable parameters and strong generalization capabilities.\n\n## 提出方法\n\n![](/img/image.png)\n<center>VRPSAM 接受point, scribble, box或者mask标注好的参考图像 </center>\n\n![](/img/image-1.png)\n\n<center>Visual reference Prompt Encoder包括Feature Augmenter和Prompt Generator</center>\n\n<br>\nFeature Augmenter中 Image encoder是固定的ResNet50, 使用reference的MaskAvgPool特征和前景mask作为提示, Prompt Generator 使用N个VRP quries首先和参考图像做交互学习分割目标知识，然后和目标图像做交互获得前景信息，最后经SA得到和SAM对齐的Prompts（听上去很玄乎）\n\n## 方法细节\n二分类损失和Dice Loss作为损失函数:\n![](/img/image-3.png)\n<br>\n实验设置： \n- Few-shot setting on COCO-20<sup>i</sup>（60 train base+20 test novel） and Pascal-5<sup>i</sup> datasets(15 train base+5 test novel), randomly sampled 1000 reference-target pairs in each fold for evaluation.\n- 使用SEEM生成point, cribble和box annotations.\n\n## 实验结果\n![](/img/image-4.png)\n![](/img/image-5.png)\n\n","tags":["CVPR2024"]}]